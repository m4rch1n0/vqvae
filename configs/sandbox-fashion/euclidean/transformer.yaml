# FashionMNIST Vanilla VAE Euclidean Transformer Training Config
# Organized structure: experiments/fashionmnist/vanilla/euclidean/transformer/

system:
  seed: 42
  device: auto

data:
  codes_path: experiments/sandbox-fashion/euclidean/codebook/codes.npy  # Sandbox path
  labels_path: experiments/sandbox-fashion/euclidean/vae/latents_train/y.pt  # Sandbox path
  batch_size: 512  # Larger batch for speed
  num_workers: 0
  vanilla_vae: true  # Use VanillaCodesDataset

training:
  epochs: 15  # Super fast!
  lr: 1e-3   # Higher LR for faster convergence
  weight_decay: 0.01
  label_smoothing: 0.0  # No label smoothing for speed

out:
  dir: experiments/sandbox-fashion/euclidean/transformer  # Sandbox output

run_name: fast_transformer_test

model:
  num_classes: 10       # FashionMNIST has 10 classes
  num_tokens: 129       # 128 codes + 1 BOS token (smaller codebook)
  embed_dim: 256        # Smaller model for speed
  n_layers: 4           # Fewer layers for speed
  n_head: 4            # Fewer heads for speed
  max_seq_len: 2        # [BOS, code]
  dropout: 0.1


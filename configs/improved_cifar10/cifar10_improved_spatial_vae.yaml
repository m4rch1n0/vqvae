# IMPROVED CIFAR-10 Spatial VAE Configuration
# Addresses: architecture depth, latent capacity, training stability

seed: 42
device: auto
max_epochs: 300  # More epochs for convergence
lr: 1e-3         # Higher learning rate for faster convergence
weight_decay: 1e-5  # Lower weight decay to avoid over-regularization
log_interval: 50
val_interval: 1
early_stop: 30   # More patience

# Progressive KL annealing for better latent space learning
kl_anneal_epochs: 150  # Much longer annealing
beta: 0.2             # Lower initial beta, will be annealed

optimizer: adamw
scheduler:
  name: cosine
  t_max: 300
grad_clip_max_norm: 1.0

# Output and tracking
out_dir: experiments/cifar10/improved/spatial/vae
save_latents: true
mlflow_tracking_uri: experiments/mlruns
experiment_name: CIFAR10_Improved_Spatial_VAE
run_name: improved_spatial_vae

# Data configuration
data:
  root: ./data
  name: CIFAR10
  batch_size: 128  # Smaller batch for better gradients
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  augment: true

# IMPROVED Spatial VAE Model
model:
  in_channels: 3
  output_image_size: 32
  
  # DOUBLED latent capacity: 64D × 4×4 = 1024D total
  latent_dim: 64  # Was 32, now 64
  
  # DEEPER architecture for CIFAR-10 complexity
  enc_channels: [64, 128, 256, 512]      # Added 512 layer
  dec_channels: [512, 256, 128, 64]      # Mirror encoder
  
  # Loss and training improvements
  recon_loss: mse
  norm_type: batch
  mse_use_sigmoid: false
  
  # Better KL regularization
  free_bits_default: 0.5      # Higher free bits threshold
  capacity_max_default: 50.0  # Higher capacity
  capacity_anneal_steps_default: 150000  # Longer annealing
  capacity_mode_default: abs

